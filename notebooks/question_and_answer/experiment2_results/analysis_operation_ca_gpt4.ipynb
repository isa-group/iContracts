{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ybdZ5XahFcM"
      },
      "source": [
        "# Analysis Customer Agreements with LLMs\n",
        "\n",
        "This notebook is designed to automate the process of analysis and evaluation by comparing GPT-4's responses to manually provided ones. The goal is to pose various questions of interest regarding the GitHub agreement to GPT-4 and to compare its answers with those manually prepared in a [preceding article](https://hdl.handle.net/11705/JCIS/2022/041).\n",
        "\n",
        "\n",
        "\n",
        "![process](img/process.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF9xoNH7m_ul"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install openai\n",
        "! pip install tiktoken\n",
        "! pip install tqdm\n",
        "! pip install chromadb\n",
        "! pip install langchain_experimental\n",
        "! pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9gPZtfWtm_uo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Union\n",
        "\n",
        "import chromadb\n",
        "\n",
        "from langchain.pydantic_v1 import Field, BaseModel\n",
        "from langchain.chains.openai_functions import create_structured_output_chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yKd2oxSrahsy"
      },
      "outputs": [],
      "source": [
        "# add your api key\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pmeaa0y3UvB"
      },
      "source": [
        "## Respond to analysis operations on the Github customer agreement\n",
        "\n",
        "In this section, we will present GPT-4 with some questions about the GitHub Customer Agreement.\n",
        "\n",
        "Why do we use GPT-4? A [technical study](https://arxiv.org/abs/2303.08774) conducted by Open AI, which extensively evaluates the performance of GPT-4, concludes that the model performs at a human-like level in professional and academic benchmark tests.\n",
        "\n",
        "*   Specifically, in two of the tests conducted that had a legal focus, the GPT-4 was more than 85% accurate, assessing skills such as critical reading, logic and analysis in this area.\n",
        "*   Other studies focused on assessing typical NLP tasks such as questioning and answering.\n",
        "\n",
        "\n",
        "In addition, there are other interesting studies in which they increase the justification of model performance. In [Sifatkaur Dhingra et al.](https://doi.org/10.1016/j.tbench.2023.100139) use a specific dataset to evaluate the accuracy of the model on a natural language understanding (NLU) task. The results show that GPT-4 achieves more than 80% accuracy on commonsense and SuperGlue, which are partially focused on tasks we are interested in, such as question-answer and paraphrase detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcyfLnEs3HeY"
      },
      "source": [
        "### Customer Agreement relevant questions\n",
        "\n",
        "26 questions collected in a [previous article](https://hdl.handle.net/11705/JCIS/2022/041)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "bx7zbB_b3IAL"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"As a customer, can I share the product or service with third parties?\",\n",
        "    \"As a customer, can I modify the source code of the product or service to my liking?\",\n",
        "    \"Can the supplier make modifications to the products or services?\",\n",
        "    \"Does the contract provide any protection for confidential customer information?\",\n",
        "    \"Does the customer retain ownership of the data he provides to the supplier?\",\n",
        "    \"Are the rights of use the supplier receives over the customer's data limited to what is strictly necessary?\",\n",
        "    \"Does the provider commit to any security standards or practices regarding customer content?\",\n",
        "    \"Are the licenses received by the supplier on the customer's intellectual property limited?\",\n",
        "    \"Does the supplier have to delete personal information after the end of the contract?\",\n",
        "    \"Does the supplier indemnify the customer for infringement of third party intellectual property?\",\n",
        "    \"Are the customer's indemnification obligations limited in third party claims?\",\n",
        "    \"Is the customer's ability to confront the supplier or any other party limited?\",\n",
        "    \"Does the supplier have a liability limit of 12 months' quota or higher?\",\n",
        "    \"What limits of liability against consequential damages does the supplier have?\",\n",
        "    \"If the supplier indemnifies for infringement of third party intellectual property, is it exempt from the limit of liability?\",\n",
        "    \"Is the customer's liability limited?\",\n",
        "    \"Does the customer have any right to terminate the agreement?\",\n",
        "    \"If the contract is self-renewing can the customer opt out at that time?\",\n",
        "    \"Does the customer have any liability to pay taxes?\",\n",
        "    \"What rights does the customer have regarding data migration?\",\n",
        "    \"Is the contract renewal automatic or does it need to be initiated by the customer?\",\n",
        "    \"What is the mandatory governing law?\",\n",
        "    \"Is the mandatory headquarters located within the United States?\",\n",
        "    \"Is the customer's ability to develop or procure similar products or services from other suppliers limited?\",\n",
        "    \"Who is responsible for ensuring that the services function properly?\",\n",
        "    \"Can the customer notify the supplier via email?\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq2WpmQNahsy"
      },
      "source": [
        "### Populate the Database using the Knowledge Base\n",
        "\n",
        "To enhance the responses generated by GPT, the RAG (Retrieval Augmented Generation) technique has been chosen. This approach involves supplying the model with relevant documents as context to better address a question.\n",
        "\n",
        "In this part of the code, the complete documents are divided by sections and stored in a database with their semantic vector representation (embedding) for later use, as can be seen in the image.\n",
        "\n",
        "During the experiment, we used 10 relevant documents out of the total number of chunks, in 2024 the CA was divided into 44 and in 2022 into 52 chunks.\n",
        "\n",
        "![rag](img/rag.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "SEZRTA5TmMBz"
      },
      "outputs": [],
      "source": [
        "client = chromadb.Client(chromadb.Settings(allow_reset=True))\n",
        "client.reset()\n",
        "\n",
        "# create collection to store github general terms documents\n",
        "collection = client.get_or_create_collection(\"github-customer-agreement\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "M-lTOT0Gahsz"
      },
      "outputs": [],
      "source": [
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "semantic_chunker = SemanticChunker(OpenAIEmbeddings())\n",
        "\n",
        "def chunk_markdown(document: str):\n",
        "    \"\"\"Split ducument based on Markdown structure\"\"\"\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    return markdown_splitter.split_text(document)\n",
        "\n",
        "def chunk_semanticly(document: str):\n",
        "    \"\"\"Split document grouping sentences semanticly closed to each other\"\"\"\n",
        "    return semantic_chunker.create_documents([document])\n",
        "\n",
        "def read_and_chunk(path: str, chunking_function=chunk_semanticly):\n",
        "    \"\"\"Read file and chunck it\n",
        "\n",
        "    Returns: a list of documents.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        document = f.read()\n",
        "    return chunking_function(document)\n",
        "\n",
        "def upsert_documents(document_id, documents):\n",
        "    \"\"\"Insert or updates a list of documents\"\"\"\n",
        "    collection.upsert(\n",
        "        documents=[document.page_content for document in documents],\n",
        "        metadatas=[document.metadata for document in documents],\n",
        "        ids=[\"{}-{}\".format(document_id, i) for i in range(len(documents))]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ZbphFtHhQhnJ"
      },
      "outputs": [],
      "source": [
        "knoledge_base = [\n",
        "    'github-general-terms.md',\n",
        "    'github-data-protection.md'\n",
        "]\n",
        "\n",
        "for document_path in knoledge_base:\n",
        "    documents = read_and_chunk(document_path, chunking_function=chunk_markdown)\n",
        "    upsert_documents(document_path, documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApmrBe5texXL",
        "outputId": "9f4e41d9-98bd-45bd-848e-409785f34ee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "collection.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8S15_1Pahs0"
      },
      "source": [
        "### Chain of Thought QA using Vector Database with Knowledge Base\n",
        "\n",
        "To address this problem two prompt engineering techniques were tested: One-Shot and Chain-of-Thought. Few-Shot was not considered as it's not well suited for this particular scenario (we don't have multiple examples to provide the LLM with in the promtp). While One-Shot prompting is less expensive, it also tends to produce more inconsistent answers and is more prone to produce hallucinations. The following list sumarizes the most relevant differences between both approaches:\n",
        "\n",
        "* One-Shot:\n",
        "    * More cost-efficent (less input tokens as well as less output tokens, wich translates into less money expenditure).\n",
        "    * Less complete answers.\n",
        "    * Incosistencies between queries.\n",
        "    * More likely to produce hallucinations.\n",
        "* [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot):\n",
        "    * More expensive.\n",
        "    * Detailed and well-reasoned answers.\n",
        "    * Consistent answer between queries.\n",
        "    * Less likely to produce hallucinations.\n",
        "\n",
        "The goal is to obtain a straightforward \"Yes\" or \"No\" response to quickly determine if the model's answers align with the manual ones. Following this, we seek an explanation of the answer and the section where the information is located. Lastly, to evaluate the answer's accuracy, we request the score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "CNORktnqm_up"
      },
      "outputs": [],
      "source": [
        "class Background(BaseModel):\n",
        "    \"\"\"Background explaining user question\"\"\"\n",
        "    background: str = Field(..., description=\"Background explaining user question\")\n",
        "\n",
        "class Thought(BaseModel):\n",
        "    \"\"\"A thought about user question\"\"\"\n",
        "    thought: str = Field(..., description=\"A thought about user question\")\n",
        "    flawed: bool = Field(..., description=\"Whether or not the thought is flawed or misleading\")\n",
        "    helpful: bool = Field(..., description=\"Whether or not the thought is helpful to solve user question\")\n",
        "\n",
        "class Answer(BaseModel):\n",
        "    \"\"\"The answer to user question\"\"\"\n",
        "    simple_answer: str = Field(..., description=\"Respond to the question with 'Yes' or 'No'\")\n",
        "    section: str = Field(..., description=\"Part of the context where the information to answer the question was found.\")\n",
        "    answer: str = Field(..., description=\"Answer to the user's question and mention the section(s) from which the answer is obtained\")\n",
        "    score: int = Field(..., description=\"Score from 1 to 10 on how correct the anwser is\", min_value=1, max_value=10)\n",
        "\n",
        "class ChainOfThoughtAnalyzer(BaseModel):\n",
        "    \"\"\"A chain of thoughts to answer user question\"\"\"\n",
        "    background: Background = Field(..., description=\"Background explaining user questions\")\n",
        "    thoughts: List[Thought] = Field(..., description=\"List of thoughts about user question\")\n",
        "    answer: Answer = Field(..., description=\"The answer to user question\")\n",
        "\n",
        "class ChainOfThoughtBuilderForQuestionAndAnswering():\n",
        "    \"\"\"Build a chain of thought (CoT) to anwser a questions using a LLM\"\"\"\n",
        "\n",
        "    def __init__(self, llm, verbose=False):\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are the most intelligent person in the world.\"\n",
        "                \"\"\n",
        "                \"You will receive a prize if you follow ALL these rules:\"\n",
        "                \"- First, establish a detailed background useful to anwser the user question.\"\n",
        "                \"- Each thought must include whether it is relevant and whether it is helpful.\"\n",
        "                \"- Continue to add thoughts until you can confidently answer the question.\"\n",
        "                \"- The anwser must be scored accurately and honestly.\"),\n",
        "            (\"human\", 'Useful context: {context}'),\n",
        "            (\"human\", 'User question: \"\"\"{question}\"\"\"'),\n",
        "        ])\n",
        "        self.chain = create_structured_output_chain(ChainOfThoughtAnalyzer, llm, self.prompt, verbose=verbose)\n",
        "\n",
        "    def build(self, question: str, context: str = None):\n",
        "        return self.chain.run(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "lXJI9J3oo05d"
      },
      "outputs": [],
      "source": [
        "def cot_analysis_to_markdown(cot: ChainOfThoughtAnalyzer, question: str, top_documents: List[str], metadatas: List[dict]):\n",
        "    \"\"\"Convert a ChainOfThought to markdown\"\"\"\n",
        "    md = \"# Top documents\\n\"\n",
        "    for i, (document, metadata) in enumerate(zip(top_documents, metadatas)):\n",
        "        md += \"## Document \" + str(i) + \": \" + \" - \".join(metadata.values()) + \"\\n\"\n",
        "        md += document + \"\\n\\n\"\n",
        "\n",
        "    md += \"# \" + question + \"\\n\"\n",
        "    md += \"## Background\\n\"\n",
        "    md += cot.background.background + \"\\n\\n\"\n",
        "\n",
        "    md += \"## Thoughts\\n\"\n",
        "    for thought in cot.thoughts:\n",
        "        md += \"- \" + thought.thought + \"\\n\"\n",
        "        md += \"  - Flawed: \" + str(thought.flawed) + \"\\n\"\n",
        "        md += \"  - Helpful: \" + str(thought.helpful) + \"\\n\\n\"\n",
        "\n",
        "    md += \"## Answer\\n\"\n",
        "    md += \"- \" + cot.answer.simple_answer + \". Section(s): \" + cot.answer.section + \". Justification: \" + cot.answer.answer + \"\\n\"\n",
        "    md += \"  - Score: \" + str(cot.answer.score) + \"\\n\\n\"\n",
        "\n",
        "    return md\n",
        "\n",
        "def search_topn_documents(query, n_results=5):\n",
        "    \"\"\"Return the top N document given a query text\"\"\"\n",
        "    return collection.query(\n",
        "        query_texts=query,\n",
        "        n_results=n_results\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZXKByj73YIt"
      },
      "source": [
        "### Main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "DFVTn9PYo5XZ"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
        "cot_builder_analysis = ChainOfThoughtBuilderForQuestionAndAnswering(llm, verbose=True)\n",
        "\n",
        "def get_GPT4_analysis_response(question, number, strategy, n_rag_documents, cot_builder_analysis):\n",
        "    \"\"\"Return the answer of the model and the question\"\"\"\n",
        "    top_documents = search_topn_documents(question, n_results=n_rag_documents)\n",
        "\n",
        "    data = zip(top_documents['documents'][0], top_documents['metadatas'][0])\n",
        "\n",
        "    context = \"\\n\".join([\"# \" + \" - \".join(metadata.values()) + \"\\n\" + document for (document, metadata) in data])\n",
        "\n",
        "    cot = cot_builder_analysis.build(question, context=context)\n",
        "    markdown = cot_analysis_to_markdown(cot, question, top_documents['documents'][0], top_documents['metadatas'][0])\n",
        "\n",
        "    directory_path = os.path.join(f\"./{strategy}\", \"analysis\")\n",
        "\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "\n",
        "    file_path = os.path.join(directory_path, f\"question_{number}_{strategy}.md\")\n",
        "\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(markdown)\n",
        "\n",
        "    return cot, question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVZkPd6t6jl8"
      },
      "source": [
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OakuwXsyAznI"
      },
      "source": [
        "## Compare results using GPT-4 as an assistant to the manual evaluation\n",
        "\n",
        "In this phase of the experiment, we present the results obtained from GPT-4 using various strategies together with those from the manual analysis. Our goal is to compare all responses as efficiently and correctly as possible, so we will perform a manual evaluation assisted by GPT-4, automating part of the evaluation and using it as a starting point.\n",
        "\n",
        "Therefore, the initial step is to ask GPT-4 to determine whether the answers of each strategy match the manual one. Using this data, we will categorize GPT-4's responses based on different patterns identified in earlier iterations of the experiment. The aim is to derive more insightful conclusions from this study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7_pccaxtIpI"
      },
      "source": [
        "### Chain of Thought compare both model and manual answer\n",
        "\n",
        "The goal is to obtain a clear \"Match\" or \"No Match\" response, as this represents the primary function we can utilize GPT-4 for as an assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "h3CMJYui1yNy"
      },
      "outputs": [],
      "source": [
        "class Thought2(BaseModel):\n",
        "    \"\"\"A thought about user question\"\"\"\n",
        "    thought: str = Field(..., description=\"A thought about user prompt\")\n",
        "    flawed: bool = Field(..., description=\"Whether or not the thought is flawed or misleading\")\n",
        "    helpful: bool = Field(..., description=\"Whether or not the thought is helpful to solve the task\")\n",
        "\n",
        "class Answer2(BaseModel):\n",
        "    \"\"\"The answer to user question\"\"\"\n",
        "    simple_answer: str = Field(..., description=\"Indicate whether the answers match by responding with 'Match' or 'Not Match'\")\n",
        "    answer: str = Field(..., description=\"Provide a justification for your assessment.\")\n",
        "    score: int = Field(..., description=\"Score from 1 to 10 on how correct your evaluation is\", min_value=1, max_value=10)\n",
        "\n",
        "class ChainOfThoughtComparator(BaseModel):\n",
        "    \"\"\"A chain of thoughts to answer user question\"\"\"\n",
        "    thoughts: List[Thought2] = Field(..., description=\"List of thoughts about user prompt\")\n",
        "    answer: Answer2 = Field(..., description=\"The answer to user prompt\")\n",
        "\n",
        "class ChainOfThoughtBuilderForComparison():\n",
        "    \"\"\"Build a chain of thought (CoT) to compare sentences\"\"\"\n",
        "\n",
        "    def __init__(self, llm, verbose=False):\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\",\n",
        "                \"You are the most intelligent person in the world.\"\n",
        "                \"You will receive a prize if you follow ALL these rules:\"\n",
        "                \"- You will be provided with one question and two answers to that question.\"\n",
        "                \"- Compare the answers to determine if they convey the same meaning.\"\n",
        "                \"- Each thought must include whether it is relevant and whether it is helpful.\"\n",
        "                \"- Continue to add thoughts until you can confidently answer the comparation.\"\n",
        "                \"- The anwser must be scored accurately and honestly.\"),\n",
        "            (\"human\", 'Useful context: {context}'),\n",
        "            (\"human\",\n",
        "             'User prompt: the question is \"\"\"{question}\"\"\"'\n",
        "             'Answer 1: \"\"\"{manual}\"\"\"'\n",
        "             'Answer 2: \"\"\"{gpt4}\"\"\"'\n",
        "             ),\n",
        "        ])\n",
        "        self.chain = create_structured_output_chain(ChainOfThoughtComparator, llm, self.prompt, verbose=verbose)\n",
        "\n",
        "    def build(self, question: str, manual: str, gpt4: str, context: str = None):\n",
        "        return self.chain.run(context=context, question=question, manual=manual, gpt4=gpt4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "a_nLgr2F2K8Q"
      },
      "outputs": [],
      "source": [
        "def cot_evaluation_to_markdown(cot: ChainOfThoughtComparator, question: str, manual_answer: str, gpt4_answer:str, strategy:str):\n",
        "    \"\"\"Convert a ChainOfThought to markdown\"\"\"\n",
        "    md = \"# Evaluation of question \"+ question + \"\\n\"\n",
        "    md += \"## Background\\n\"\n",
        "    md += \"- Manual analysis answer is: \" + manual_answer + \"\\n\\n\"\n",
        "    md += \"- GPT-4 analysis answer with the strategy \"  + strategy + \" is: \" + gpt4_answer + \"\\n\"\n",
        "\n",
        "    md += \"## Thoughts\\n\"\n",
        "    for thought in cot.thoughts:\n",
        "        md += \"- \" + thought.thought + \"\\n\"\n",
        "        md += \"  - Flawed: \" + str(thought.flawed) + \"\\n\"\n",
        "        md += \"  - Helpful: \" + str(thought.helpful) + \"\\n\\n\"\n",
        "\n",
        "    md += \"## Assesment\\n\"\n",
        "    md += \"- \" + cot.answer.simple_answer + \". Justification: \" + cot.answer.answer + \"\\n\"\n",
        "    md += \"  - Score: \" + str(cot.answer.score) + \"\\n\\n\"\n",
        "\n",
        "    return md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FGdQwgLyie4"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "nth4z07BzKZa"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
        "cot_builder_evaluation = ChainOfThoughtBuilderForComparison(llm, verbose=True)\n",
        "\n",
        "def get_GPT4_evaluation_response(question, manual_answer, gpt4_answer, number, strategy, cot_builder_evaluation):\n",
        "    \"\"\"Return the assesment of the model\"\"\"\n",
        "\n",
        "    cot = cot_builder_evaluation.build(question=question, manual=manual_answer, gpt4=gpt4_answer, context=None)\n",
        "    markdown = cot_evaluation_to_markdown(cot, question, manual_answer, gpt4_answer, strategy)\n",
        "    directory_path = os.path.join(f\"./{strategy}\", \"evaluation\")\n",
        "\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "\n",
        "    file_path = os.path.join(directory_path, f\"question_{number}_{strategy}.md\")\n",
        "\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(markdown)\n",
        "\n",
        "    return cot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zinc8cIOs5PL"
      },
      "source": [
        "## Execution of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "RoSTM-O_ist5"
      },
      "outputs": [],
      "source": [
        "strategies = [\"gpt4_2024_complete\", \"gpt4_2024_rag\", \"gpt4_2022_complete\", \"gpt4_2022_rag\"]\n",
        "manual_answers = [\n",
        "    \"No, as the customer may neither distribute the product to third parties nor upgrade parts of the product respectively (section 1.12 Restrictions).\",\n",
        "    \"No, as the customer may neither distribute the product to third parties nor upgrade parts of the product respectively (section 1.12 Restrictions).\",\n",
        "    \"Yes, because GitHub has the right to make changes to products if they do not materially diminish the functionality of the product.\",\n",
        "    \"Yes, because confidential information will only be used for the relationship between both parties (section 4.3 Protection on Confidential information).\",\n",
        "    \"Yes, because the customer retains all rights, title and interest and GitHub may only use the customer's data for what is strictly necessary (Data Protection Document, Nature of Data Processing and Ownership and Processing for GitHub's legitimate business operations sections).\",\n",
        "    \"Yes, because the customer retains all rights, title and interest and GitHub may only use the customer's data for what is strictly necessary (Data Protection Document, Nature of Data Processing and Ownership and Processing for GitHub's legitimate business operations sections).\",\n",
        "    \"Yes, because GitHub complies with the security standards (data protection document, Data Security section).\",\n",
        "    \"Yes, because the customer retains all rights, title and interest and GitHub may only use the customer's data for what is strictly necessary (Data Protection Document, Nature of Data Processing and Ownership and Processing for GitHub's legitimate business operations sections).\",\n",
        "    \"No, because customer data is only destroyed at the customer's request and if lawful (data protection document, Data Retention and Deletion section).\",\n",
        "    \"Yes, because if GitHub is unable to resolve a misappropriation claim, it offers customer compensation options (section 6 Third-party claims).\",\n",
        "    \"No, since the parties mutually defend each other and will contribute the amount of any final judgment (section 6 Third party claims).\",\n",
        "    \"No, as no related information appears in the CA.\",\n",
        "    \"Yes, as GitHub will not exceed the amount the customer has paid for the product during the 12 months prior to the incident (secBon 7.1 (a) Products).\",\n",
        "    \"In response to Q17 we can say that liability for indirect damages is excluded for both parties (section 7.1 (C) Exclusions).\",\n",
        "    \"Yes, as infringement of IP rights is an exception to limitation of liability and liability is limited to direct damages respectively (section 7 Limitation of liability).\",\n",
        "    \"Yes, as infringement of IP rights is an exception to limitation of liability and liability is limited to direct damages respectively (section 7 Limitation of liability).\",\n",
        "    \"Yes, as a customer may terminate the agreement, but with 30 days' notice (section 9 Term and termination).\",\n",
        "    \"Yes, as a customer may terminate the agreement, but with 30 days' notice (section 9 Term and termination).\",\n",
        "    \"Yes, the customer has tax liabilities (section 8.3 Taxes).\",\n",
        "    \"The customer can request data migration up to 90 days after the end of the agreement (section 9.4 Migration).\",\n",
        "    \"Needs to be initiated, according to CA product licenses expire at the end of the subscription period, unless renewed (section 1.2 Duration of licenses).\",\n",
        "    \"Depends on the principal office of the customer, as if it is located in the European Union, the European Economic Area or Switzerland then the agreement is governed by the laws of Ireland, otherwise by the laws of the State of California and the federal laws of the United States (section 10.11 Applicable law and venue).\",\n",
        "    \"No, as there is insufficient information in the CA on the compulsory seat.\",\n",
        "    \"No (section 10.1 Independent contracts).\",\n",
        "    \"GitHub is responsible for the performance of the services (section 10.12 GitHub and contractors).\",\n",
        "    \"Yes, the customer can notify the supplier via email (section 10.10 Notices).\"\n",
        "    ]\n",
        "numbers = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "rIIm97qTs-P8"
      },
      "outputs": [],
      "source": [
        "def main(question, number, manual_answer, strategy, n_rag_documents, cot_builder_analysis, cot_builder_evaluation):\n",
        "  analysis_results = get_GPT4_analysis_response(question, number, strategy, n_rag_documents, cot_builder_analysis)\n",
        "  gpt4_answer = analysis_results[0].answer.simple_answer + \". \" + analysis_results[0].answer.answer\n",
        "  evaluation_results = get_GPT4_evaluation_response(question, manual_answer, gpt4_answer, number, strategy, cot_builder_evaluation)\n",
        "  return analysis_results, evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = main(questions[25], numbers[25], manual_answers[25], strategies[2], 10, cot_builder_analysis, cot_builder_evaluation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
